{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXOVbT26utyJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzXk7dIaYJG2",
        "outputId": "e2b5ec64-0613-4a4d-a380-255ab0449ca4"
      },
      "outputs": [],
      "source": [
        "def parse_text_file(file_path):\n",
        "    descriptions = []\n",
        "    criminals = []\n",
        "    current_description = None\n",
        "    current_criminal = None\n",
        "    parsing_description = False\n",
        "\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()  # Remove any leading/trailing whitespace\n",
        "\n",
        "            if line.startswith(\"**DESCRIPTION**\"):\n",
        "                # Start a new description\n",
        "                current_description = \"\"\n",
        "                parsing_description = True\n",
        "                current_criminal = None\n",
        "\n",
        "            elif line.startswith(\"**CRIMINAL**\") and parsing_description:\n",
        "                # Store the criminal name associated with the current description\n",
        "                if current_description is not None:\n",
        "                    criminal_name = line[12:].strip()\n",
        "                    descriptions.append(current_description)\n",
        "                    criminals.append(criminal_name)\n",
        "                    parsing_description = False\n",
        "\n",
        "            elif parsing_description:\n",
        "                # Append to the current description\n",
        "                if current_description is not None:\n",
        "                    current_description += line + \" \"  # Add the line to the current description\n",
        "\n",
        "    return descriptions, criminals\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"data.txt\"  # Update with your file path\n",
        "crime_descriptions, criminal_names = parse_text_file(file_path)\n",
        "\n",
        "for i in crime_descriptions:\n",
        "  print(i)\n",
        "\n",
        "print()\n",
        "\n",
        "for i in criminal_names:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6LCCAMCCA6H",
        "outputId": "275e6ded-d951-4baa-cc2f-45540d4639dd"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(criminal_names)\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_descriptions, test_descriptions, train_labels, test_labels = train_test_split(\n",
        "    crime_descriptions, encoded_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Save the label encoder\n",
        "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
        "\n",
        "# Prepare dataset class\n",
        "class CrimeDataset(Dataset):\n",
        "    def __init__(self, descriptions, labels, tokenizer, max_length):\n",
        "        self.descriptions = descriptions\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.descriptions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        description = str(self.descriptions[idx])\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            description,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "assert len(criminal_names) == len(crime_descriptions), \"Mismatch in labels and descriptions\"\n",
        "# Set up BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(set(encoded_labels)))\n",
        "\n",
        "# Set training parameters\n",
        "batch_size = 8\n",
        "max_length = 256\n",
        "epochs = 10\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# Prepare train dataset and data loader\n",
        "train_dataset = CrimeDataset(train_descriptions, train_labels, tokenizer, max_length)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Prepare test dataset and data loader\n",
        "test_dataset = CrimeDataset(test_descriptions, test_labels, tokenizer, max_length)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Set up optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "loss_list = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{epochs}'):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    loss_list.append(avg_loss)\n",
        "    print(f'Train Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Evaluate the model on test data\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_dataloader, desc=f'Eval Epoch {epoch + 1}/{epochs}'):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            predicted_labels = torch.argmax(logits, dim=1)\n",
        "\n",
        "            all_preds.extend(predicted_labels.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Compute evaluation metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    print(f\"Test Epoch {epoch + 1}/{epochs} -- Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Save the trained model\n",
        "model.save_pretrained('crime_bert_model')\n",
        "tokenizer.save_pretrained('crime_bert_tokenizer')\n",
        "\n",
        "print(\"Training complete. Model saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "eQsdnHwRDnh7",
        "outputId": "f5431b05-a461-48c6-d106-cc110e819927"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = list(range(1, len(loss_list) + 1))  # One value per epoch\n",
        "y = loss_list\n",
        "\n",
        "plt.plot(x, y, marker='o')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Epochs vs Loss Graph\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DBKV-srClb2",
        "outputId": "647e5bee-5c5b-4cc4-f4ef-56c3df3e61d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Criminal: Mr. Daniel Thompson\n"
          ]
        }
      ],
      "source": [
        "# Load saved model, tokenizer, and label encoder\n",
        "model_path = 'crime_bert_model'  # Path to the saved model\n",
        "tokenizer_path = 'crime_bert_tokenizer'  # Path to the saved tokenizer\n",
        "label_encoder_path = 'label_encoder.pkl'  # Path to the saved label encoder\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
        "label_encoder = joblib.load(label_encoder_path)\n",
        "\n",
        "# Function to predict criminal from description\n",
        "def predict_criminal(description):\n",
        "    encoded_input = tokenizer(description, return_tensors='pt', max_length=128, truncation=True)\n",
        "    input_ids = encoded_input['input_ids']\n",
        "    attention_mask = encoded_input['attention_mask']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predicted_label = torch.argmax(logits, dim=1).item()\n",
        "    predicted_criminal = label_encoder.inverse_transform([predicted_label])[0]\n",
        "\n",
        "    return predicted_criminal\n",
        "\n",
        "# Example usage\n",
        "description = \"\"\"In the serene town of Meadowbrook, a sinister crime unfolded one foggy evening. The victim, Mr. James Monroe, a prominent\n",
        "local politician known for his charismatic charm, was discovered dead in his sprawling estate. The scene was haunting, with tendrils of mist\n",
        "creeping through the corridors and casting eerie shadows on the walls. Mr. Monroe lay motionless in his study, a look of disbelief frozen on his face.\n",
        "As investigators delved into the mystery, a diverse cast of characters emerged among the guests who had attended Mr. Monroe's fateful gathering.\n",
        "Dr. Gregory Westwood, a renowned surgeon with a penchant for secrecy, arrived early, his demeanor guarded and inscrutable. Ms. Samantha Taylor,\n",
        "a fiery journalist known for her relentless pursuit of truth, mingled with the crowd, her probing questions sparking suspicion. Mr. Daniel Thompson,\n",
        "a wealthy entrepreneur with a reputation for ruthlessness, moved through the room with ease, his steely gaze betraying little of his true intentions.\n",
        "Each guest harbored their own secrets and motivations, their interactions fraught with tension and hidden agendas.\n",
        "As the night wore on, tempers flared and long-held grudges bubbled to the surface. Amidst heated arguments and whispered accusations, someone seized\n",
        "a lethal weapon—a vintage revolver—and fired a single fatal shot, ending Mr. Monroe's life in an instant.\n",
        "The murder weapon, a tarnished revolver of antique design, was discovered discarded in the garden, its barrel still warm from the fatal shot.\n",
        "Yet, amidst the chaos and confusion, the identity of the killer remained elusive, shrouded in a veil of mystery.\n",
        "Each guest's tangled web of lies and deceit provided investigators with a labyrinth of clues, but unraveling the truth would require navigating\n",
        "the treacherous depths of deception that lurked within Meadowbrook's elite circles.\"\"\"\n",
        "\n",
        "predicted_criminal = predict_criminal(description) #Should predict Daniel Thompson\n",
        "\n",
        "print(\"Predicted Criminal:\", predicted_criminal)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
